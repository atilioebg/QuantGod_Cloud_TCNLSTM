# QuantGod Cloud Infrastructure ‚òÅÔ∏è

Este diret√≥rio cont√©m o pipeline de ETL (Extract, Transform, Load) projetado para processar terabytes de dados de Orderbook (L2) na nuvem (RunPod) de forma eficiente, utilizando streaming de dados e otimiza√ß√£o de mem√≥ria.

---

## üìÇ Estrutura de Arquivos e Pastas

### 1. `configs/` (Configura√ß√µes)
Arquivos YAML que definem o comportamento do pipeline.
*   **`cloud_config.yaml`**: Configura√ß√£o oficial para produ√ß√£o no RunPod. Aponta para o diret√≥rio de dados montado via `rclone`.
*   **`test_local.yaml`**: Configura√ß√£o para testes em ambiente de desenvolvimento. Aponta para pastas locais (`data/L2/raw/l2_samples`).

**Par√¢metros Principais:**
*   `paths.rclone_mount`: Caminho do mount do Google Drive.
*   `paths.processed_output`: Destino dos arquivos `.parquet`.
*   `etl.orderbook_levels`: N√≠vel do **Hard Cut** (Ex: 200).
*   `features.apply_zscore`: Ativa/Desativa a normaliza√ß√£o estat√≠stica.

### 2. `etl/` (M√≥dulos de Processamento)
O motor do processamento, dividido em responsabilidades modulares:

*   **`extract.py`**: Implementa a l√≥gica **Zero-Copy**. Ele abre os ZIPs diretamente do mount e l√™ o conte√∫do (JSON/CSV) linha por linha em buffer de mem√≥ria, sem nunca descompactar arquivos no disco f√≠sico do RunPod.
*   **`transform.py`**: O c√©rebro do pipeline.
    *   Reconstr√≥i o Orderbook a partir de snapshots e deltas.
    *   Aplica o **Hard Cut 200** (mant√©m estritamente os top 200 n√≠veis).
    *   Realiza amostragem temporal (1s ticks) e resampling (1min OHLCV).
    *   Calcula Micro-Price, Spread e IOBI.
    *   Aplica **Stationarity Fix** (Log-Returns para pre√ßos e Log1p para volume).
*   **`load.py`**: Gerencia a persist√™ncia. Utiliza o formato **Apache Parquet** com compress√£o **Snappy** para garantir leitura ultra-r√°pida durante o treino do modelo.
*   **`validate.py`**: Garante a qualidade do dado. Verifica se h√° NaNs, valores infinitos, se a ordem cronol√≥gica est√° correta e se existem "gaps" de tempo excessivos.

### 3. `orchestration/` (Coordena√ß√£o)
*   **`run_pipeline.py`**: O ponto de entrada. Ele coordena o fluxo entre todos os m√≥dulos acima. Suporta a passagem de arquivos de config via terminal:
    `python -m src.cloud.pre_processamento.orchestration.run_pipeline src/cloud/pre_processamento/configs/test_local.yaml`

### 4. `setup_cloud.sh` (Automa√ß√£o de Ambiente)
Script bash para preparar a inst√¢ncia Linux (RunPod).
*   Instala pacotes do sistema (`rclone`, `python3-pip`).
*   Cria o ambiente virtual `.venv`.
*   Instala as depend√™ncias de Python.
*   Cria a √°rvore de diret√≥rios oficial (`data/L2/pre_processed`, `data/artifacts`, etc.).

---

## üöÄ Como Usar na Cloud (RunPod)

Para rodar o processamento completo dos anos 2023-2026, siga estes passos ajustados para o ambiente Ubuntu 24.04:

### Passo 1: Preparar a m√°quina
```bash
# Execute o script de setup (ele criar√° .venv, pastas de dados/logs e instalar√° depend√™ncias)
chmod +x src/cloud/pre_processamento/setup_cloud.sh
./src/cloud/pre_processamento/setup_cloud.sh
```

### Passo 2: Configurar e Ativar o Rclone
No Linux (RunPod), o mount √© feito em um diret√≥rio do sistema:

1. **Configurar Credenciais**:
   * O arquivo `rclone.conf` no storage provavelmente est√° vazio.
   * `nano /workspace/rclone.conf`
   * Cole o conte√∫do do seu `rclone.conf` local (que come√ßa com `[drive]`).
   * Salve (Ctrl+O, Enter) e saia (Ctrl+X).

2. **Montar o Drive**:
   ```bash
   mkdir -p /workspace/mnt/gdrive
   rclone mount drive: /workspace/mnt/gdrive --config /workspace/rclone.conf --daemon --vfs-cache-mode writes
   ```
   *Verifique com `ls /workspace/mnt/gdrive` se suas pastas apareceram.*

3. **Ajustar Caminhos**:
   * No arquivo `src/cloud/pre_processamento/configs/cloud_config.yaml`, verifique se o `rclone_mount` aponta corretamente para a pasta montada. Ex: `rclone_mount: "/workspace/mnt/gdrive/PROJETOS/BTC_USDT_L2_2023_2026"`.

### Passo 3: Rodar o Processamento (Modo Persistente)
Como o processamento pode levar horas, use o `tmux` para garantir que o script continue rodando mesmo se voc√™ fechar o navegador.

1. **Entrar no tmux**:
   ```bash
   tmux new -s pilar_etl
   ```

2. **Ativar Ambiente e Disparar**:
   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$PYTHONPATH:/workspace
   python3 src/cloud/pre_processamento/orchestration/run_pipeline.py
   ```

3. **Comandos √öteis do tmux**:
   * **Desconectar (Sair sem parar)**: `Ctrl + B`, solte, e aprete `D`.
   * **Reconectar**: `tmux attach -t pilar_etl`.
   * **Navegar nos logs**: `Ctrl + B`, solte, e aprete `[` para usar as setas (esc para sair).

---

## üíª Como Usar Local (Windows)

#### 1. Montar o Google Drive
Como o WinFSP j√° est√° instalado, use o `rclone.exe` na raiz:
```powershell
.\rclone.exe mount drive: Z: --vfs-cache-mode full --config rclone.conf
```
*Mantenha o terminal aberto.*

#### 2. Rodar Testes Locais
```bash
source .venv/bin/activate
python -m src.cloud.pre_processamento.orchestration.run_pipeline src/cloud/pre_processamento/configs/test_local.yaml
```

---

## üõ†Ô∏è Requisitos T√©cnicos (`requirements.txt`)
O pipeline depende de:
*   `polars` / `pandas`: Processamento de dados de alta performance.
*   `pyarrow`: Engine para escrita de Parquet.
*   `scikit-learn`: Para aplica√ß√£o do `StandardScaler` (Z-Score).
*   `tqdm`: Barras de progresso para monitoramento de grandes volumes.
*   `pyyaml`: Leitura dos arquivos de configura√ß√£o.
*   `pytest`: Execu√ß√£o da su√≠te de testes de integridade.

---

## üß™ Valida√ß√£o e Testes

Para garantir que a migra√ß√£o para a nuvem n√£o corrompa a integridade dos dados, implementamos uma su√≠te de testes autom√°ticos que valida a estrutura dos arquivos Parquet gerados.

### O que √© validado:
- **Shape e Colunas**: Verifica se o arquivo cont√©m as 810 colunas (Op√ß√£o B - 200 n√≠veis).
- **Ordena√ß√£o do Book**: Garante que Bids est√£o em ordem decrescente e Asks em crescente.
- **Spread Positivo**: Valida que o melhor Bid √© sempre menor que o melhor Ask (sem book cruzado).
- **Qualidade das Features**: Certifica-se de que n√£o existem NaNs ou Infs nas 9 features de treinamento.
- **Continuidade Temporal**: Verifica se os dados est√£o em ordem cronol√≥gica e sem gaps inesperados.

### Como rodar os testes:
```bash
pytest tests/test_cloud_etl_output.py
```
