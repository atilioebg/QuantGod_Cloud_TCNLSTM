# QuantGod Cloud Infrastructure ‚òÅÔ∏è

Este diret√≥rio cont√©m o pipeline de ETL (Extract, Transform, Load) projetado para processar terabytes de dados de Orderbook (L2) na nuvem (RunPod) de forma eficiente, utilizando streaming de dados e otimiza√ß√£o de mem√≥ria.

---

## üìÇ Estrutura de Arquivos e Pastas

### 1. `configs/` (Configura√ß√µes)
Arquivos YAML que definem o comportamento do pipeline.
*   **`cloud_config.yaml`**: Configura√ß√£o oficial para produ√ß√£o no RunPod. Aponta para o diret√≥rio de dados montado via `rclone`.
*   **`test_local.yaml`**: Configura√ß√£o para testes em ambiente de desenvolvimento. Aponta para pastas locais (`data/L2/raw/l2_samples`).

**Par√¢metros Principais:**
*   `paths.rclone_mount`: Caminho do mount do Google Drive.
*   `paths.processed_output`: Destino dos arquivos `.parquet`.
*   `etl.orderbook_levels`: N√≠vel do **Hard Cut** (Ex: 200).
*   `features.apply_zscore`: Ativa/Desativa a normaliza√ß√£o estat√≠stica.

### 2. `etl/` (M√≥dulos de Processamento)
O motor do processamento, dividido em responsabilidades modulares:

*   **`extract.py`**: Implementa a l√≥gica **Zero-Copy**. Ele abre os ZIPs diretamente do mount e l√™ o conte√∫do (JSON/CSV) linha por linha em buffer de mem√≥ria, sem nunca descompactar arquivos no disco f√≠sico do RunPod.
*   **`transform.py`**: O c√©rebro do pipeline.
    *   Reconstr√≥i o Orderbook a partir de snapshots e deltas.
    *   Aplica o **Hard Cut 200** (mant√©m estritamente os top 200 n√≠veis).
    *   Realiza amostragem temporal (1s ticks) e resampling (1min OHLCV).
    *   Calcula Micro-Price, Spread e IOBI.
    *   Aplica **Stationarity Fix** (Log-Returns para pre√ßos e Log1p para volume).
*   **`load.py`**: Gerencia a persist√™ncia. Utiliza o formato **Apache Parquet** com compress√£o **Snappy** para garantir leitura ultra-r√°pida durante o treino do modelo.
*   **`validate.py`**: Garante a qualidade do dado. Verifica se h√° NaNs, valores infinitos, se a ordem cronol√≥gica est√° correta e se existem "gaps" de tempo excessivos.

### 3. `orchestration/` (Coordena√ß√£o)
*   **`run_pipeline.py`**: O ponto de entrada. Ele coordena o fluxo entre todos os m√≥dulos acima. Suporta a passagem de arquivos de config via terminal:
    `python -m src.cloud.pre_processamento.orchestration.run_pipeline src/cloud/pre_processamento/configs/test_local.yaml`

### 4. `setup_cloud.sh` (Automa√ß√£o de Ambiente)
Script bash para preparar a inst√¢ncia Linux (RunPod).
*   Instala pacotes do sistema (`rclone`, `python3-pip`).
*   Cria o ambiente virtual `.venv`.
*   Instala as depend√™ncias de Python.
*   Cria a √°rvore de diret√≥rios oficial (`data/L2/pre_processed`, `data/artifacts`, etc.).

---

## üöÄ Como Usar

### Passo 1: Preparar a m√°quina
```bash
cd cloud
chmod +x setup_cloud.sh
./setup_cloud.sh
```

### Passo 2: Configurar e Ativar o Rclone
O processo de ativa√ß√£o depende do seu ambiente:

#### 1. No Windows (Seu PC atual)
Como o WinFSP j√° est√° instalado, voc√™ pode usar o `rclone.exe` presente na raiz do projeto para montar o Google Drive como um disco local (`Z:`):
1. Abra um terminal exclusivo para o rclone.
2. Execute o comando para montar:
   ```powershell
   .\rclone.exe mount drive: Z: --vfs-cache-mode full --config rclone.conf
   ```
3. **Mantenha o terminal aberto.** Se fechar, o disco `Z:` ser√° desconectado.

#### 2. Na Cloud (Linux / RunPod)
No Linux, o mount √© feito em um diret√≥rio do sistema:
1. Configure o acesso (caso ainda n√£o tenha feito): `rclone config`.
2. Crie a pasta de destino: `mkdir -p /workspace/gdrive`.
3. Ative o mount em segundo plano:
   ```bash
   rclone mount drive: /workspace/gdrive --vfs-cache-mode full --allow-other &
   ```
   *O `&` no final libera o terminal para outros comandos.*

### Passo 3: Rodar o Processamento
Ative o ambiente e execute o pipeline:
```bash
source .venv/bin/activate
# Para produ√ß√£o (RunPod):
python -m src.cloud.pre_processamento.orchestration.run_pipeline
# Para testes (Local):
python -m src.cloud.pre_processamento.orchestration.run_pipeline src/cloud/pre_processamento/configs/test_local.yaml
```

---

## üõ†Ô∏è Requisitos T√©cnicos (`requirements.txt`)
O pipeline depende de:
*   `polars` / `pandas`: Processamento de dados de alta performance.
*   `pyarrow`: Engine para escrita de Parquet.
*   `scikit-learn`: Para aplica√ß√£o do `StandardScaler` (Z-Score).
*   `tqdm`: Barras de progresso para monitoramento de grandes volumes.
*   `pyyaml`: Leitura dos arquivos de configura√ß√£o.
*   `pytest`: Execu√ß√£o da su√≠te de testes de integridade.

---

## üß™ Valida√ß√£o e Testes

Para garantir que a migra√ß√£o para a nuvem n√£o corrompa a integridade dos dados, implementamos uma su√≠te de testes autom√°ticos que valida a estrutura dos arquivos Parquet gerados.

### O que √© validado:
- **Shape e Colunas**: Verifica se o arquivo cont√©m as 810 colunas (Op√ß√£o B - 200 n√≠veis).
- **Ordena√ß√£o do Book**: Garante que Bids est√£o em ordem decrescente e Asks em crescente.
- **Spread Positivo**: Valida que o melhor Bid √© sempre menor que o melhor Ask (sem book cruzado).
- **Qualidade das Features**: Certifica-se de que n√£o existem NaNs ou Infs nas 9 features de treinamento.
- **Continuidade Temporal**: Verifica se os dados est√£o em ordem cronol√≥gica e sem gaps inesperados.

### Como rodar os testes:
```bash
pytest tests/test_cloud_etl_output.py
```
